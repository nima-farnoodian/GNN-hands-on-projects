{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on PyG document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting .\\ml-latest-small.zip\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import download_url, extract_zip\n",
    "\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "movie_path = './ml-latest-small/movies.csv'\n",
    "rating_path = './ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movie=pd.read_csv(movie_path)\n",
    "rating=pd.read_csv(rating_path)\n",
    "movie.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of genres associated with a movie: 10\n"
     ]
    }
   ],
   "source": [
    "mx=-float(\"inf\")\n",
    "for genre in list(movie.genres):\n",
    "    gen=genre.split(\"|\")\n",
    "    if len(gen)> mx:\n",
    "        mx=len(gen)\n",
    "print(\"The maximum number of genres associated with a movie:\",mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of times a user A rates a movie B: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of times a user A rates a movie B:\", max(rating.groupby(by=[\"userId\",\"movieId\"]).count()['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_node_csv(path, index_col, encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, index_col=index_col, **kwargs)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "\n",
    "    x = None\n",
    "    if encoders is not None:\n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    return x, mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.76214531e-01  1.20601252e-01 -2.93624073e-01 -2.29858026e-01\n",
      " -8.22923928e-02  2.37709522e-01  3.39984864e-01 -7.80964196e-01\n",
      "  1.18127614e-01  1.63373962e-01 -1.37715712e-01  2.40282789e-01\n",
      "  4.25125599e-01  1.72417849e-01  1.05279692e-01  5.18164098e-01\n",
      "  6.22218400e-02  3.99285793e-01 -1.81652278e-01 -5.85578680e-01\n",
      "  4.49722409e-02 -1.72750309e-01 -2.68443495e-01 -1.47386149e-01\n",
      " -1.89217970e-01  1.92150578e-01 -3.83842468e-01 -3.96007091e-01\n",
      "  4.30648863e-01 -3.15320134e-01  3.65949631e-01  6.05158620e-02\n",
      "  3.57325703e-01  1.59736529e-01 -3.00983816e-01  2.63250291e-01\n",
      " -3.94311100e-01  1.84855521e-01 -3.99549276e-01 -2.67889529e-01\n",
      " -5.45117497e-01 -3.13403942e-02 -4.30644333e-01  1.33278117e-01\n",
      " -1.74793795e-01 -4.35465544e-01 -4.77379113e-01  7.12555572e-02\n",
      " -7.37001151e-02  5.69137156e-01 -2.82579720e-01  5.24975285e-02\n",
      " -8.20007861e-01  1.98296756e-01  1.69511825e-01  2.71780342e-01\n",
      "  2.64610827e-01 -2.55737714e-02 -1.74096107e-01  1.63314253e-01\n",
      " -3.95260930e-01 -3.17556299e-02 -2.62556046e-01  3.52754712e-01\n",
      "  3.01434875e-01 -1.47197291e-01  2.10075796e-01 -1.84010491e-01\n",
      " -4.12896037e-01  4.14775789e-01 -1.89769492e-01 -1.35482445e-01\n",
      " -3.79272133e-01 -4.68020439e-02 -3.33601385e-02  9.00394097e-02\n",
      " -3.30133140e-01 -3.87316942e-02  3.75082314e-01 -1.46996319e-01\n",
      "  4.34959829e-01  5.38325727e-01 -2.65445173e-01  1.64445907e-01\n",
      "  4.17078644e-01 -4.72508594e-02 -7.48731196e-02 -4.26261097e-01\n",
      " -1.96994558e-01  6.10316209e-02 -4.74262655e-01 -6.48334742e-01\n",
      "  3.71462464e-01  2.50957102e-01  1.22529611e-01  8.88766572e-02\n",
      " -1.06724210e-01  5.33984490e-02  9.74507183e-02 -3.46660167e-02\n",
      " -1.02882817e-01  2.32289001e-01 -2.53739536e-01 -5.13112307e-01\n",
      "  1.85216278e-01 -3.04357797e-01 -3.55209075e-02 -1.26975372e-01\n",
      " -7.71632940e-02 -5.15330076e-01 -2.28071719e-01  2.03343164e-02\n",
      "  7.38175958e-02 -1.52558655e-01 -4.00837570e-01 -2.47749180e-01\n",
      "  3.97470325e-01 -2.60260701e-01  2.50906169e-01  1.68228924e-01\n",
      "  1.33900508e-01 -2.10833233e-02 -4.70035732e-01  4.78850156e-01\n",
      "  2.80345589e-01 -4.64546800e-01  3.21747035e-01  2.34207422e-01\n",
      "  2.45772451e-01 -4.71482307e-01  5.00400960e-01  4.10190076e-01\n",
      "  5.15216827e-01  2.62549460e-01  2.11593546e-02 -3.89687568e-01\n",
      " -2.41742760e-01 -2.14834630e-01 -8.62650797e-02 -1.65323570e-01\n",
      " -5.21895029e-02  3.41874868e-01  4.50314462e-01 -3.06973577e-01\n",
      " -2.02294186e-01  6.85521722e-01 -5.33892572e-01  3.58471543e-01\n",
      "  1.45286605e-01 -7.07056001e-02 -1.50529072e-01 -8.56279582e-02\n",
      " -7.67851025e-02  1.89544857e-01 -1.04067773e-01  5.33544004e-01\n",
      " -5.27887225e-01  2.42332090e-02 -2.64348090e-01 -2.23186895e-01\n",
      " -3.81208718e-01  7.59914368e-02 -4.64485109e-01 -3.36549252e-01\n",
      "  4.21229839e-01  1.07479207e-01  1.90457791e-01  2.89487489e-03\n",
      " -1.08513705e-01  1.53545350e-01  3.16023648e-01 -2.70840749e-02\n",
      " -5.40594459e-01  8.97286758e-02 -1.15549676e-01  3.97803992e-01\n",
      " -4.97683346e-01 -2.84893364e-01  4.99861799e-02  3.61279696e-01\n",
      "  6.90535665e-01  1.46821439e-01  1.73396602e-01 -1.74582347e-01\n",
      " -3.15702260e-01  6.72999769e-02  2.17250243e-01  9.78535116e-02\n",
      " -1.29472464e-01 -1.86929435e-01  1.34878129e-01 -1.53885290e-01\n",
      "  7.44715557e-02 -1.85536250e-01 -2.80628383e-01 -1.14144213e-01\n",
      "  4.12249625e-01  6.39491975e-02 -1.45715117e-01 -9.82065052e-02\n",
      " -1.33081883e-01 -1.88410461e-01 -2.84838937e-02 -3.49510163e-02\n",
      "  3.34258713e-02  6.98896796e-02  1.90354511e-01 -2.96724051e-01\n",
      "  2.64706067e-03  1.09140947e-01  1.70892701e-02  2.60589242e-01\n",
      "  3.29038620e-01 -6.61560148e-02  2.39665717e-01 -2.26194620e-01\n",
      " -3.36869545e-02  1.49400130e-01 -3.21265638e-01 -2.68577904e-01\n",
      "  5.72632015e-01 -4.92308497e-01  2.00666577e-01 -3.49261820e-01\n",
      " -2.89886612e-02  6.09010458e-01 -5.72333157e-01  2.35000670e-01\n",
      "  6.47180574e-03 -3.14952508e-02  2.78108083e-02 -3.90340954e-01\n",
      " -2.08950117e-01 -3.04452837e-01 -7.20199272e-02 -8.29840004e-02\n",
      "  3.73792857e-01  7.38937110e-02 -2.21076086e-02  9.88139287e-02\n",
      " -1.51426882e-01 -1.40430734e-01  2.26017952e-01  2.76089966e-01\n",
      " -8.87747630e-02 -1.12816028e-01 -2.66286045e-01  2.77834296e-01\n",
      " -4.75609973e-02  6.71005547e-02 -2.78584175e-02 -2.39991937e-02\n",
      "  2.51708686e-01  4.68793899e-01 -5.39325476e-01  1.10598475e-01\n",
      " -3.44947308e-01  4.15990084e-01  7.28483498e-02 -3.19647521e-01\n",
      "  4.90374565e-01 -7.30331149e-03 -2.64252443e-03  9.63711083e-01\n",
      "  3.23884904e-01 -7.79616535e-02 -2.37589255e-01  2.34038591e-01\n",
      " -3.16053987e-01 -1.65628293e-03 -1.09070671e+00  3.38409364e-01\n",
      "  4.70607281e-02  1.07435413e-01 -2.06672356e-01  4.26446088e-03\n",
      " -1.38461241e-03 -5.31455636e-01 -2.75648654e-01 -1.64648622e-01\n",
      " -3.42916757e-01 -4.26118672e-01  6.01811945e-01  4.55971897e-01\n",
      " -2.72701889e-01 -3.45802940e-02  2.62752354e-01 -6.34185225e-03\n",
      "  2.79631346e-01 -2.53559053e-01 -1.68626338e-01  3.82934660e-02\n",
      "  2.07763135e-01 -4.31525975e-01 -7.24000558e-02 -1.26854450e-01\n",
      "  2.07029749e-02  5.74441731e-01  3.54672432e-01  9.28300545e-02\n",
      "  6.70508668e-02  1.11520678e-01 -1.86510980e-02  4.62352097e-01\n",
      "  2.72504926e-01 -3.60474110e-01  5.29415369e-01 -1.00318261e-03\n",
      " -8.81360695e-02  1.49975494e-01  5.25862724e-02  4.63517487e-01\n",
      " -3.96831542e-01  2.42640823e-01 -2.08912537e-01  3.65672290e-01\n",
      " -4.73500433e-04  5.33963263e-01 -1.97879612e-01  3.11582744e-01\n",
      " -6.96715057e-01 -4.29500550e-01 -4.49359566e-01 -2.71372199e-02\n",
      " -6.98710978e-02  2.06174582e-01 -1.57107800e-01  4.43521202e-01\n",
      " -6.74267337e-02 -3.00924033e-01  5.14859557e-01  3.36029828e-01\n",
      "  6.63376600e-02 -1.15235247e-01 -2.95982286e-02  2.79471755e-01\n",
      " -3.48201916e-02 -7.29324743e-02 -4.58472408e-02  1.54262856e-01\n",
      "  8.09356570e-01  5.20328224e-01 -4.02114749e-01 -3.23152021e-02\n",
      " -1.10364027e-01  7.50505254e-02 -1.51098579e-01  8.45740080e-01\n",
      " -1.80843890e-01  3.22573632e-01  1.04708321e-01  3.19663644e-01\n",
      " -1.55085236e-01  1.69236735e-01 -2.56996632e-01  2.01208770e-01\n",
      "  1.77393183e-01 -2.74333179e-01 -3.36944580e-01  5.02356887e-01\n",
      " -1.18357182e-01 -2.01166973e-01 -5.36485910e-01 -7.69810379e-02\n",
      "  1.15382867e-02 -2.36464664e-01 -2.98771430e-02  1.31366640e-01\n",
      "  2.94184387e-01  9.90917012e-02 -5.43897867e-01  1.40812770e-01\n",
      "  3.66998672e-01  5.04861325e-02  1.99122518e-01 -2.80674815e-01\n",
      "  4.34192061e-01 -1.40274823e-01  5.78048944e-01  1.77715778e-01\n",
      "  8.98364484e-02  3.29651892e-01  6.13008775e-02 -3.24933589e-01]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 0.32208762 -0.00123908  0.17937377 -0.36919138 -0.06460274  0.09153692\n",
      "  0.24119096 -0.29494217  0.07728957  0.11577005 -0.04479983  0.17928234\n",
      "  0.1475363   0.21511652  0.36810791  0.20910913  0.27194238  0.34880087\n",
      " -0.57251936 -0.18253218  0.4448957   0.27452925  0.04266282 -0.07683562\n",
      "  0.18689147  0.4496505  -0.16932622 -0.24896334 -0.20479265  0.40285036\n",
      " -0.2101927   0.03775701  0.07848521  0.12848447  0.02593089  0.4715598\n",
      "  0.17853785 -0.07379771  0.08130724 -0.23328738 -0.49801245 -0.04135723\n",
      " -0.12094605  0.17028998 -0.19154078 -0.38459808 -0.77479136 -0.10622733\n",
      " -0.2304489   0.4024145  -0.8745089   0.23853712 -0.4712986   0.21262182\n",
      "  0.3340935  -0.24154    -0.1483509  -0.14513564 -0.34830925 -0.08349245\n",
      " -0.69097275 -0.29845262 -0.12230504  0.07482646 -0.18775596 -0.3754651\n",
      "  0.21369492 -0.10096409 -0.12234445  0.31431514 -0.23989926  0.22460794\n",
      "  0.0399599   0.36034834 -0.5663802   0.21883512  0.11020288 -0.10870823\n",
      "  0.07084075 -0.02608179  0.18370324  0.08465949 -0.20478249 -0.24435617\n",
      " -0.08180565 -0.01903111 -0.03591372  0.02398443 -0.2855857   0.07374766\n",
      " -0.29744208 -0.87717843  0.47101936 -0.0494047   0.36394492  0.482644\n",
      "  0.01564615  0.03558917 -0.26203    -0.1121847   0.0241104   0.37477782\n",
      " -0.09897303 -0.09851858  0.15000843  0.00689534 -0.12652436 -0.3159893\n",
      "  0.31449488 -0.2942561  -0.2694104   0.20221162  0.14329888 -0.19584627\n",
      " -0.3410443  -0.03172762  0.7365027   0.31923506  0.2438129   0.30732602\n",
      "  0.09933245  0.19010943 -0.10694525  0.05178664  0.03233436 -0.10314636\n",
      "  0.2649922   0.31206465  0.43152618 -0.6426122   0.0840958  -0.0432735\n",
      " -0.04991193 -0.12718563  0.13789187  0.01306224  0.34383258  0.09234294\n",
      " -0.09922738 -0.52159905  0.25842273 -0.01057126 -0.00478189  0.03938858\n",
      "  0.19086099  0.32933885 -0.24345161 -0.07328319 -0.39280015  0.14541808\n",
      "  0.32839534 -0.04184634  0.07407122 -0.7386053  -0.09076015  0.15802307\n",
      " -0.09780021 -0.21605958 -0.30027467  0.2323658   0.01072446  0.49570465\n",
      "  0.04974837  0.29931435 -0.05382248  0.35328114  0.3419176   0.49667245\n",
      " -0.4860523  -0.19098845  0.8154575   0.22962622 -0.32077783 -0.32726687\n",
      " -0.367717    0.3452114  -0.02620159 -0.14315048  0.10648432 -0.24638046\n",
      " -0.09366632  0.17198639 -0.08508814  0.20120296 -0.05879208 -0.34020975\n",
      " -0.19565329  0.2828087   0.20124306 -0.08207255  0.09779122 -0.26375008\n",
      "  0.12176559 -0.01041479 -0.4385985   0.11058219  0.48010394 -0.10981981\n",
      " -0.6375458   0.29336807 -0.1920764   0.46536973  0.2704201   0.19388463\n",
      "  0.17379023 -0.3007702  -0.0275121  -0.02291276  0.3678463   0.02492153\n",
      "  0.53705496  0.18851233 -0.13344418  0.08917347  0.05542957 -0.24818316\n",
      " -0.04199777  0.05767386 -0.18278812 -0.41686475  0.16070595 -0.46362543\n",
      "  0.11769217 -0.3770692   0.02960374  0.692561   -0.4830891   0.21128401\n",
      "  0.18214527 -0.18429579  0.0681766  -0.02460879 -0.19073623 -0.06736984\n",
      " -0.5670072  -0.23929311 -0.08497227  0.03093951  0.31079894  0.1291628\n",
      "  0.05248259 -0.3344983   0.18810134  0.23547177 -0.00183485  0.45361614\n",
      "  0.24885082 -0.05641065 -0.2977458  -0.43511704 -0.07969435 -0.17670156\n",
      " -0.13347092  0.1938272   0.22002596 -0.11057543  0.26473755 -0.27179065\n",
      "  0.03410878 -0.4771442   0.44719058 -0.05570416  0.39643747  0.27483267\n",
      "  0.33305615 -0.10890252  0.2788817   0.21596934 -0.05252272 -0.35867548\n",
      " -0.69062907  0.03960182  0.00652785 -0.01095348 -0.10027702  0.04770013\n",
      " -0.34146932 -0.16714163  0.07136464 -0.1807847  -0.30248472 -0.68428737\n",
      " -0.09592848 -0.2141112  -0.6552438   0.5675644   0.26946738 -0.00190061\n",
      "  0.8618065   0.16771556  0.03102748 -0.2677305  -0.07830263 -0.48510885\n",
      " -0.26737222 -0.33354253 -0.5738254   0.35678256  0.08993588 -0.13057196\n",
      " -0.1513651  -0.06124125 -0.13037089  0.5585606   0.614175   -0.04804054\n",
      " -0.0638859   0.08390605 -0.25143692 -0.04359839 -0.18525787  0.04693348\n",
      " -0.34380865 -0.09738468  0.1683365   0.0752685   0.1769452   0.17727177\n",
      " -0.03423442  0.14993554 -0.13773166 -0.20949684 -0.6127283   0.3781397\n",
      "  0.3901828  -0.08359346  0.03152128  0.13122386  0.38826075  0.21844254\n",
      "  0.0972431   0.42089358 -0.3264124  -0.26933426 -0.39095107 -0.22648653\n",
      " -0.32020715 -0.16287427 -0.03581636  0.363739    0.18583307 -0.0291401\n",
      " -0.46577957  0.2916888   0.37251312 -0.23726626  0.00338617  0.41540965\n",
      "  0.03300428  0.4500395  -0.08159234  0.33990335  0.24497904  0.0235242\n",
      " -0.1464306  -0.12644552  0.31128627 -0.15182619  0.01009398  0.49108496\n",
      "  0.14362407  0.11589024 -0.23236984  0.2475176   0.1836449  -0.24836856\n",
      " -0.11220913 -0.23113322  0.0842896  -0.24378659  0.13307258  0.42355725\n",
      "  0.33348376 -0.3437014   0.0344367   0.18795514  0.20037197 -0.05355961\n",
      "  0.2848529   0.0717658   0.05487169 -0.08103789  0.27076873  0.11700248]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.58979344 -0.23598255 -0.25411725  0.00311624 -0.08485737 -0.26799768\n",
      " -0.07506651 -0.3002136   0.05151652  0.16585363  0.26076776  0.38256362\n",
      "  0.43732867 -0.09301949 -0.26568803 -0.09716298 -0.48096094  0.11878292\n",
      "  0.13675483  0.04712067 -0.23696537 -0.52332336 -0.01631867  0.06127304\n",
      " -0.7433302  -0.11898906 -0.7886529  -0.48108855  0.10314927 -0.32372454\n",
      "  0.8144374  -0.39774537 -0.50315547 -0.7972457  -0.6324822   0.32320985\n",
      " -0.38419437 -0.11186695 -0.1324357   0.02069666 -0.14309539 -0.0370119\n",
      "  0.06116579  0.16332883 -0.11174309  0.25234267 -1.0464071  -0.37252343\n",
      "  0.15601997 -0.29991606  0.19883864  0.2343344  -0.3702577   0.31733564\n",
      "  0.84428644  0.06977719  0.03273663  0.09948339 -0.31141308  0.50517714\n",
      "  0.00309272  0.38013652  0.04582737  0.00633379 -0.00142918 -0.13568659\n",
      " -0.07611365 -0.25844312 -0.8022129   0.5508589  -0.09124386 -0.21782017\n",
      " -0.78810936 -0.5118384   0.46672547  0.55274725 -0.37124714 -0.18645377\n",
      "  0.3585697  -0.19586323  0.18042535 -0.42548886 -0.09681422 -0.05536837\n",
      "  0.52489287  0.24481142  0.01934664 -0.29637936 -0.1277783  -0.30534947\n",
      "  0.45349374  0.07469101 -0.07061689  0.2624302   0.37383935  0.14306359\n",
      "  0.00127857 -0.41776088 -0.24014093 -0.2509353   0.34843782  0.31144044\n",
      "  0.0808733  -0.5764053   0.54085284 -0.01802203 -0.12959798 -0.07399664\n",
      "  0.3936979   0.6488388  -0.02030003 -0.5665558   0.2967598   0.520002\n",
      "  0.21538728  0.10369676  0.06199208  0.01896283 -0.15269236 -1.0642662\n",
      "  0.7614961   0.20734386  0.44718924  0.14493968  0.6580228  -0.09440905\n",
      " -0.23316365  0.4215707   0.1195763  -0.32571068  0.16425563 -0.49508703\n",
      " -0.19516118 -0.56183213 -0.14933276  0.610941   -0.17897959 -0.01805552\n",
      " -0.5964053   0.04918591  0.15347804 -0.42829406  0.73295283 -0.35291103\n",
      " -0.11159656  0.06127812 -0.29704392  0.4396664  -0.09660351  0.65579444\n",
      " -0.61403424  0.02576606  0.43827453  0.01733282 -0.40002275 -0.08178308\n",
      " -0.37126926  0.08230279 -0.13104396 -0.5326111  -0.29928368  0.6993657\n",
      " -0.04398742 -0.15702991  0.09794132 -0.03017466 -0.10002708  0.199966\n",
      " -0.4818854   0.17949156  0.5656603  -0.11954784 -0.696373    0.05259641\n",
      " -0.00549608  0.16739355 -0.31692895 -0.09747531  0.33193663  0.4719962\n",
      "  0.12653954  0.19130926  0.42949092  0.55291235  0.31463274 -0.31433088\n",
      " -0.41508684  0.32897702  0.35702732 -0.1920966   0.22239415 -0.48717856\n",
      "  0.3409156  -0.22137432 -0.1266758   0.21120834 -0.31347895  0.8468937\n",
      "  0.20112668 -0.42598733  0.5131572  -1.2351419   0.76971793 -0.17414267\n",
      " -0.02181136 -0.0356865  -1.105949   -0.5720654   0.05585196  0.12461495\n",
      " -0.45065832  0.06428951 -0.16033873  0.39932927 -0.10322935 -0.02025502\n",
      " -0.18010448  0.06234786 -0.02188883 -0.15795426  0.28316957  0.02385282\n",
      "  0.03098137 -0.07853306  0.29896578 -0.06237327  0.5498678   0.17862315\n",
      "  0.2116474   0.44483367  0.04890747 -0.162381   -0.22669895  0.18871985\n",
      "  0.07943622  0.1359757  -0.18484493  1.113551    0.82809544 -0.31202707\n",
      "  0.09505999  0.05096091  0.38804898  0.25000468  0.558486    0.31088772\n",
      " -0.05318568 -0.07675371  0.15282278  0.09189964 -0.01429148  0.6657543\n",
      " -0.03346021 -0.44703493  0.80067486 -0.4799278   0.17478174 -0.30563813\n",
      "  0.5536521   0.42380962  0.48674306 -0.49677992 -0.45194814 -0.95563084\n",
      " -0.20709987 -0.22605716 -0.0099914   0.98797685  0.5880775   0.08305439\n",
      " -0.5578132   0.21136862 -0.3607222   0.52668494  0.33983573 -0.15756187\n",
      "  0.00423779 -0.05354516 -0.5777671   0.5595104  -0.05747148  0.16837652\n",
      "  0.37946853 -0.25776428  0.08421484 -0.15229936 -0.03280768  0.10083867\n",
      " -0.41858304 -0.44499016 -0.29309887  0.6144206   0.08548218 -0.06349564\n",
      " -0.6152552   0.79544085 -0.24058406  0.2063889  -0.5125261   0.6312013\n",
      "  0.36744294 -0.4400988   0.46913967  0.23087728 -0.13737966  0.21696861\n",
      "  0.40043226 -0.02490607 -1.139676    0.02653918 -0.32730213  0.09984124\n",
      "  0.05725667 -0.84722155  0.06451946  0.45698035  0.63563025  0.45185634\n",
      " -0.2751906   0.21346165  0.17374253  0.42822042 -0.6584535   0.4000258\n",
      " -0.02035557 -0.6730788  -1.0269238   0.16877282 -0.09248707 -0.79977626\n",
      "  0.38093373  0.5171233   0.04200954 -0.04867526 -0.18772238  0.16339499\n",
      " -0.21974911  0.21939285  0.03676502 -0.29750267 -0.37409678 -0.52095085\n",
      " -0.41314605 -0.489477   -0.8189662   0.08531482  0.34576944  0.12505981\n",
      "  0.24945222 -0.25254658 -0.03156116  0.27573124 -0.60857177  0.3357\n",
      "  0.22913106  0.6607082  -0.30215803 -0.05315317  0.22247504  0.06138719\n",
      "  0.33555198 -0.0848518   0.08764573  0.10872053 -0.40389338 -0.14949782\n",
      "  0.19458489 -0.81060654  0.79730946 -0.41162547  0.01364165  0.23472963\n",
      " -0.09732277 -0.29044035  0.03843231 -0.07090472 -0.17404465 -0.44859377\n",
      " -0.31867278  0.4165608  -0.05431673  0.14036179  1.0559161   0.5301814 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(object):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', device=None):\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, df):\n",
    "        x = self.model.encode(df.values, show_progress_bar=True,\n",
    "                              convert_to_tensor=True, device=self.device)\n",
    "        return x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenresEncoder(object):\n",
    "    def __init__(self, sep='|'):\n",
    "        self.sep = sep\n",
    "\n",
    "    def __call__(self, df):\n",
    "        genres = set(g for col in df.values for g in col.split(self.sep))\n",
    "        mapping = {genre: i for i, genre in enumerate(genres)}\n",
    "\n",
    "        x = torch.zeros(len(df), len(mapping))\n",
    "        for i, col in enumerate(df.values):\n",
    "            for genre in col.split(self.sep):\n",
    "                x[i, mapping[genre]] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cea7c4c66294f1aa6169b3eced519e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=305.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "movie_x, movie_mapping = load_node_csv(\n",
    "    movie_path, index_col='movieId', encoders={\n",
    "        'title': SequenceEncoder(),\n",
    "        'genres': GenresEncoder()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9742, 404])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.2835e-02,  5.3030e-02,  5.3577e-02, -2.7935e-02,  1.6134e-02,\n",
       "         1.2132e-02,  2.4147e-02,  2.0295e-02, -5.5472e-03,  1.3974e-02,\n",
       "         4.2932e-02,  5.0252e-02,  2.6336e-03,  5.3083e-02,  1.0653e-02,\n",
       "         2.6884e-02,  1.5536e-02,  4.0110e-02,  1.5220e-02, -9.0487e-02,\n",
       "         4.0381e-02, -4.1801e-03,  3.8394e-02, -1.7117e-02, -1.0566e-01,\n",
       "         1.8147e-02,  3.2558e-03, -4.7458e-02, -1.1269e-01, -3.1918e-02,\n",
       "         1.1425e-02,  2.4668e-02, -1.9351e-02, -6.9325e-02, -1.2954e-02,\n",
       "        -2.1300e-02, -4.1694e-02, -5.4402e-02,  5.9398e-02, -2.4313e-02,\n",
       "        -1.3531e-02, -5.9771e-02,  5.5338e-02, -2.7162e-02,  8.4955e-02,\n",
       "         1.4287e-02, -8.2289e-02, -4.7492e-02,  3.2746e-02,  2.7004e-02,\n",
       "        -1.0257e-01, -2.9592e-02,  7.4494e-02, -7.6777e-02,  5.6812e-02,\n",
       "         2.4909e-02,  2.7727e-02,  2.7807e-02,  4.4575e-02, -6.6666e-02,\n",
       "         5.2070e-02,  1.5745e-03,  2.6283e-02, -5.5703e-03,  8.5972e-02,\n",
       "        -4.1571e-02, -1.8053e-02, -2.7062e-02, -2.7583e-02, -3.0702e-02,\n",
       "         3.2445e-03,  8.3563e-02, -4.6668e-02, -4.3490e-02,  2.3026e-04,\n",
       "        -2.3606e-02,  7.0184e-02,  3.7391e-02,  2.5190e-04,  1.6817e-02,\n",
       "        -6.4290e-02, -6.8840e-02, -4.4606e-02, -6.1110e-02, -6.2255e-02,\n",
       "         5.0663e-03,  8.3985e-02,  8.5769e-02, -3.8029e-02,  9.7192e-02,\n",
       "        -1.3675e-01, -4.1168e-02,  3.7325e-02,  8.8508e-02, -5.5783e-02,\n",
       "        -4.1161e-02, -3.6833e-02, -1.1317e-01, -1.4078e-02,  5.0386e-02,\n",
       "        -3.0050e-02, -6.2885e-03,  8.3741e-02, -3.2639e-02,  3.7135e-02,\n",
       "        -4.7837e-02, -1.8224e-02, -6.6495e-02,  3.2504e-02,  2.9357e-03,\n",
       "        -3.1190e-02, -1.6150e-02, -8.5173e-03,  7.6083e-02, -2.1674e-02,\n",
       "        -3.0052e-03, -1.4325e-02, -2.5553e-02, -5.8902e-03,  6.0146e-03,\n",
       "         1.5644e-01,  2.8301e-02, -6.0704e-02,  5.1123e-02, -5.2196e-02,\n",
       "        -3.7234e-02,  2.3845e-02, -5.4289e-33,  2.9928e-02, -7.2600e-02,\n",
       "         9.5505e-02,  5.2868e-02,  3.0283e-02,  4.0689e-02, -3.6959e-02,\n",
       "         5.7527e-02, -7.7190e-02,  1.2528e-02, -1.2578e-01, -5.7814e-02,\n",
       "        -9.1189e-02, -2.7366e-02,  4.6654e-02, -4.0573e-02, -8.6643e-02,\n",
       "         1.3140e-03,  8.9958e-02,  3.4189e-02, -1.9601e-02,  4.7115e-02,\n",
       "        -7.9926e-02, -1.7943e-02, -6.2343e-03,  6.7183e-03,  2.5204e-02,\n",
       "         8.8327e-03,  4.4563e-02,  9.5603e-03, -2.0989e-03, -1.3945e-03,\n",
       "         3.7799e-02, -1.2521e-02,  9.4486e-02, -1.7559e-02,  1.7603e-02,\n",
       "        -3.7835e-02,  3.3703e-03,  1.5551e-02,  5.9491e-02, -3.1448e-02,\n",
       "        -6.7958e-02,  4.3979e-02, -2.8117e-02,  4.4236e-02,  3.4613e-03,\n",
       "        -2.0872e-02, -2.1875e-02,  7.5602e-02, -9.4616e-02,  7.7594e-02,\n",
       "        -5.6064e-02, -7.4572e-02, -5.7023e-02,  3.5232e-02,  7.0428e-02,\n",
       "        -5.7028e-02,  1.0948e-02,  4.6610e-03,  1.0508e-01,  2.3883e-02,\n",
       "         3.4959e-02, -4.5514e-02, -4.4807e-02,  3.0387e-02,  9.9302e-02,\n",
       "        -9.8618e-02,  6.3670e-02,  3.6774e-02, -3.4566e-02,  2.8839e-02,\n",
       "         3.1404e-02, -1.2376e-01, -4.7010e-03,  4.9536e-02,  4.5418e-02,\n",
       "         3.3945e-02, -8.5702e-02, -6.7516e-02, -7.7632e-03,  4.2492e-02,\n",
       "        -3.8355e-02, -1.5350e-02,  3.6329e-02, -2.9153e-02, -3.6366e-02,\n",
       "        -2.7372e-02,  1.0481e-02,  4.9572e-03,  8.9183e-05, -4.1753e-02,\n",
       "        -7.3542e-02, -4.0719e-03,  5.8637e-02,  2.1430e-33,  2.4208e-02,\n",
       "        -4.8385e-02, -5.8001e-03,  1.1449e-02,  4.9476e-02, -1.3555e-02,\n",
       "        -5.6392e-02,  1.0440e-01,  2.0588e-02,  4.5390e-02, -3.8288e-02,\n",
       "        -6.0449e-02,  3.7094e-02,  2.0694e-02,  8.1260e-02,  9.1385e-02,\n",
       "        -2.0418e-02, -2.8103e-02,  9.0283e-03,  3.8159e-02, -7.0158e-03,\n",
       "        -3.0076e-02, -4.6904e-03,  8.3662e-03, -6.8545e-02,  5.0226e-02,\n",
       "         4.1017e-02, -4.3210e-02, -9.6235e-02, -6.0026e-02, -1.0048e-02,\n",
       "        -5.7274e-02,  3.3833e-02,  7.5860e-02, -3.2374e-02, -3.8997e-02,\n",
       "         2.2469e-02, -8.5476e-02, -2.0560e-03, -8.3580e-02,  3.7215e-02,\n",
       "        -2.0290e-02, -1.9778e-02,  1.1004e-01, -5.7364e-02, -2.3441e-02,\n",
       "        -1.1018e-02,  1.8766e-03,  7.7082e-02,  1.1058e-01, -5.0167e-02,\n",
       "        -2.7922e-02, -7.0666e-03, -1.0732e-01, -6.0351e-02,  6.8697e-02,\n",
       "         4.2044e-02,  4.3051e-03,  7.8062e-02,  5.9778e-02,  2.5271e-02,\n",
       "         4.7923e-04, -4.4331e-02, -2.6887e-02, -1.2264e-01, -2.0088e-02,\n",
       "         9.1840e-03, -2.6539e-02,  8.0297e-02, -4.2228e-02,  7.6249e-03,\n",
       "         1.0834e-01,  7.9262e-02, -2.0289e-02, -3.7004e-02,  3.4728e-02,\n",
       "        -2.8353e-02, -1.2591e-02,  3.0131e-02, -2.5152e-02,  4.5781e-02,\n",
       "         6.6043e-03,  6.7382e-02,  3.3511e-02, -4.7349e-02,  2.8352e-02,\n",
       "         8.5762e-04,  9.1487e-02, -3.2281e-02,  2.3846e-02, -2.4791e-02,\n",
       "         9.3416e-02,  2.1725e-02,  2.5786e-02,  1.8099e-02, -1.5355e-08,\n",
       "         1.6313e-02,  2.1927e-02, -6.7074e-02, -5.8897e-02,  1.3802e-02,\n",
       "         5.3180e-02, -3.2459e-02,  8.5178e-03, -3.3713e-02,  4.0481e-02,\n",
       "        -9.0149e-03, -1.2747e-03, -8.0076e-03,  3.1240e-02, -9.8633e-03,\n",
       "         4.6767e-02, -2.6510e-02,  3.8408e-02,  5.0948e-03,  7.8837e-02,\n",
       "         3.3031e-02,  6.2714e-02,  3.3454e-02,  3.0723e-02, -9.1852e-02,\n",
       "         2.8742e-02, -1.0766e-01, -3.4628e-02,  4.1190e-02,  3.6500e-03,\n",
       "         2.2956e-02,  7.8558e-02, -3.1726e-02, -3.6134e-02, -5.1356e-02,\n",
       "        -3.6907e-02, -5.3789e-03,  1.4839e-03, -6.9208e-03, -5.5751e-02,\n",
       "         1.0489e-01, -1.1921e-02,  1.2361e-02, -1.5729e-02, -1.1997e-02,\n",
       "         4.5903e-03,  1.3033e-02, -8.3835e-02,  8.1507e-03,  9.8015e-02,\n",
       "        -9.0405e-02,  2.4395e-03, -1.3494e-01,  6.8702e-02,  2.9772e-02,\n",
       "        -1.0131e-02,  7.9226e-03, -4.7207e-02, -6.0298e-02,  6.0763e-03,\n",
       "         2.8819e-03,  2.2616e-02,  5.3814e-02,  1.0297e-01,  1.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, user_mapping = load_node_csv(rating_path, index_col='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "data['user'].num_nodes = len(user_mapping)  # Users do not have any features.\n",
    "data['movie'].x = movie_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
    "                  encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, **kwargs)\n",
    "\n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "\n",
    "    edge_attr = None\n",
    "    if encoders is not None:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    return edge_index, edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityEncoder(object):\n",
    "    def __init__(self, dtype=None):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, edge_label = load_edge_csv(\n",
    "    rating_path,\n",
    "    src_index_col='userId',\n",
    "    src_mapping=user_mapping,\n",
    "    dst_index_col='movieId',\n",
    "    dst_mapping=movie_mapping,\n",
    "    encoders={'rating': IdentityEncoder(dtype=torch.long)},\n",
    ")\n",
    "\n",
    "data['user', 'rates', 'movie'].edge_index = edge_index\n",
    "data['user', 'rates', 'movie'].edge_label = edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    edge_label=[100836, 1]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4],\n",
       "        [4],\n",
       "        [4],\n",
       "        ...,\n",
       "        [5],\n",
       "        [5],\n",
       "        [3]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user','rates','movie'].edge_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Lens : Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Extracting \\data\\MovieLens\\raw\\ml-latest-small.zip\n",
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4619aa48d42644d9ac7a4fdb71e0ff88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=305.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = MovieLens(\"/data/MovieLens\", model_name='all-MiniLM-L6-v2')\n",
    "data = dataset[0].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add user node features for message passing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
       "  \u001b[1muser\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 80670],\n",
       "    edge_label=[80670],\n",
       "    edge_label_index=[2, 80670]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 80670] }\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "weight = weight.max() / weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to lazy initialization, we need to run one model step so the number\n",
    "# of parameters can be inferred:\n",
    "with torch.no_grad():\n",
    "    model.encoder(train_data.x_dict, train_data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
       "  \u001b[1muser\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 80670],\n",
       "    edge_label=[80670],\n",
       "    edge_label_index=[2, 80670]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 80670] }\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[ 560,  560,  433,  ...,  406,   79,  602],\n",
       "        [1055, 7620, 2195,  ..., 2226, 6414, 2233]]), 'edge_label': tensor([4, 3, 3,  ..., 5, 4, 2]), 'edge_label_index': tensor([[ 560,  560,  433,  ...,  406,   79,  602],\n",
       "        [1055, 7620, 2195,  ..., 2226, 6414, 2233]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_data['user','rates', 'movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'movie'].edge_label_index)\n",
    "    target = train_data['user', 'movie'].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    #pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 17.7010, Train: 3.0638, Val: 3.0579, Test: 3.0648\n",
      "Epoch: 002, Loss: 14.6256, Train: 2.4824, Val: 2.4788, Test: 2.4866\n",
      "Epoch: 003, Loss: 10.1050, Train: 1.4062, Val: 1.4055, Test: 1.4152\n",
      "Epoch: 004, Loss: 6.0586, Train: 1.5468, Val: 1.5308, Test: 1.5307\n",
      "Epoch: 005, Loss: 13.8226, Train: 1.0817, Val: 1.0765, Test: 1.0843\n",
      "Epoch: 006, Loss: 7.4966, Train: 1.4881, Val: 1.4875, Test: 1.4978\n",
      "Epoch: 007, Loss: 6.0237, Train: 1.9552, Val: 1.9532, Test: 1.9627\n",
      "Epoch: 008, Loss: 7.2789, Train: 2.1923, Val: 2.1893, Test: 2.1983\n",
      "Epoch: 009, Loss: 8.3667, Train: 2.2599, Val: 2.2565, Test: 2.2656\n",
      "Epoch: 010, Loss: 8.7197, Train: 2.2127, Val: 2.2096, Test: 2.2189\n",
      "Epoch: 011, Loss: 8.4523, Train: 2.0748, Val: 2.0724, Test: 2.0823\n",
      "Epoch: 012, Loss: 7.7458, Train: 1.8549, Val: 1.8536, Test: 1.8644\n",
      "Epoch: 013, Loss: 6.8074, Train: 1.5669, Val: 1.5670, Test: 1.5788\n",
      "Epoch: 014, Loss: 5.9528, Train: 1.2669, Val: 1.2679, Test: 1.2805\n",
      "Epoch: 015, Loss: 5.6361, Train: 1.0768, Val: 1.0770, Test: 1.0888\n",
      "Epoch: 016, Loss: 6.1079, Train: 1.0376, Val: 1.0364, Test: 1.0466\n",
      "Epoch: 017, Loss: 6.6980, Train: 1.0356, Val: 1.0352, Test: 1.0461\n",
      "Epoch: 018, Loss: 6.4705, Train: 1.0878, Val: 1.0893, Test: 1.1024\n",
      "Epoch: 019, Loss: 5.7663, Train: 1.2441, Val: 1.2462, Test: 1.2604\n",
      "Epoch: 020, Loss: 5.3800, Train: 1.4345, Val: 1.4361, Test: 1.4503\n",
      "Epoch: 021, Loss: 5.4598, Train: 1.5780, Val: 1.5790, Test: 1.5930\n",
      "Epoch: 022, Loss: 5.7076, Train: 1.6414, Val: 1.6420, Test: 1.6562\n",
      "Epoch: 023, Loss: 5.8388, Train: 1.6212, Val: 1.6219, Test: 1.6365\n",
      "Epoch: 024, Loss: 5.7396, Train: 1.5264, Val: 1.5276, Test: 1.5430\n",
      "Epoch: 025, Loss: 5.4471, Train: 1.3772, Val: 1.3793, Test: 1.3956\n",
      "Epoch: 026, Loss: 5.1083, Train: 1.2124, Val: 1.2155, Test: 1.2321\n",
      "Epoch: 027, Loss: 4.9155, Train: 1.0860, Val: 1.0897, Test: 1.1058\n",
      "Epoch: 028, Loss: 4.9646, Train: 1.0282, Val: 1.0320, Test: 1.0468\n",
      "Epoch: 029, Loss: 5.0948, Train: 1.0210, Val: 1.0249, Test: 1.0397\n",
      "Epoch: 030, Loss: 5.0280, Train: 1.0541, Val: 1.0581, Test: 1.0744\n",
      "Epoch: 031, Loss: 4.7581, Train: 1.1338, Val: 1.1373, Test: 1.1555\n",
      "Epoch: 032, Loss: 4.5252, Train: 1.2354, Val: 1.2381, Test: 1.2575\n",
      "Epoch: 033, Loss: 4.4586, Train: 1.3145, Val: 1.3164, Test: 1.3364\n",
      "Epoch: 034, Loss: 4.4797, Train: 1.3408, Val: 1.3423, Test: 1.3628\n",
      "Epoch: 035, Loss: 4.4509, Train: 1.3069, Val: 1.3084, Test: 1.3294\n",
      "Epoch: 036, Loss: 4.3137, Train: 1.2258, Val: 1.2278, Test: 1.2486\n",
      "Epoch: 037, Loss: 4.1217, Train: 1.1322, Val: 1.1350, Test: 1.1543\n",
      "Epoch: 038, Loss: 3.9934, Train: 1.0691, Val: 1.0729, Test: 1.0893\n",
      "Epoch: 039, Loss: 3.9912, Train: 1.0533, Val: 1.0576, Test: 1.0718\n",
      "Epoch: 040, Loss: 4.0042, Train: 1.0722, Val: 1.0764, Test: 1.0912\n",
      "Epoch: 041, Loss: 3.9015, Train: 1.1227, Val: 1.1258, Test: 1.1434\n",
      "Epoch: 042, Loss: 3.7640, Train: 1.1925, Val: 1.1944, Test: 1.2148\n",
      "Epoch: 043, Loss: 3.7266, Train: 1.2482, Val: 1.2494, Test: 1.2712\n",
      "Epoch: 044, Loss: 3.7548, Train: 1.2671, Val: 1.2682, Test: 1.2903\n",
      "Epoch: 045, Loss: 3.7540, Train: 1.2495, Val: 1.2512, Test: 1.2724\n",
      "Epoch: 046, Loss: 3.7047, Train: 1.2146, Val: 1.2176, Test: 1.2366\n",
      "Epoch: 047, Loss: 3.6623, Train: 1.1869, Val: 1.1916, Test: 1.2077\n",
      "Epoch: 048, Loss: 3.6761, Train: 1.1774, Val: 1.1832, Test: 1.1976\n",
      "Epoch: 049, Loss: 3.7050, Train: 1.1838, Val: 1.1896, Test: 1.2045\n",
      "Epoch: 050, Loss: 3.6793, Train: 1.2051, Val: 1.2102, Test: 1.2272\n",
      "Epoch: 051, Loss: 3.6283, Train: 1.2345, Val: 1.2387, Test: 1.2579\n",
      "Epoch: 052, Loss: 3.6130, Train: 1.2539, Val: 1.2577, Test: 1.2781\n",
      "Epoch: 053, Loss: 3.6175, Train: 1.2475, Val: 1.2516, Test: 1.2720\n",
      "Epoch: 054, Loss: 3.5979, Train: 1.2161, Val: 1.2210, Test: 1.2403\n",
      "Epoch: 055, Loss: 3.5561, Train: 1.1764, Val: 1.1826, Test: 1.2002\n",
      "Epoch: 056, Loss: 3.5308, Train: 1.1463, Val: 1.1536, Test: 1.1697\n",
      "Epoch: 057, Loss: 3.5292, Train: 1.1339, Val: 1.1415, Test: 1.1575\n",
      "Epoch: 058, Loss: 3.5183, Train: 1.1392, Val: 1.1466, Test: 1.1637\n",
      "Epoch: 059, Loss: 3.4885, Train: 1.1566, Val: 1.1633, Test: 1.1821\n",
      "Epoch: 060, Loss: 3.4673, Train: 1.1726, Val: 1.1790, Test: 1.1990\n",
      "Epoch: 061, Loss: 3.4617, Train: 1.1733, Val: 1.1799, Test: 1.2003\n",
      "Epoch: 062, Loss: 3.4515, Train: 1.1551, Val: 1.1624, Test: 1.1823\n",
      "Epoch: 063, Loss: 3.4293, Train: 1.1277, Val: 1.1363, Test: 1.1551\n",
      "Epoch: 064, Loss: 3.4139, Train: 1.1105, Val: 1.1200, Test: 1.1380\n",
      "Epoch: 065, Loss: 3.4079, Train: 1.1096, Val: 1.1196, Test: 1.1376\n",
      "Epoch: 066, Loss: 3.3932, Train: 1.1198, Val: 1.1298, Test: 1.1485\n",
      "Epoch: 067, Loss: 3.3759, Train: 1.1328, Val: 1.1429, Test: 1.1622\n",
      "Epoch: 068, Loss: 3.3646, Train: 1.1408, Val: 1.1512, Test: 1.1708\n",
      "Epoch: 069, Loss: 3.3550, Train: 1.1390, Val: 1.1502, Test: 1.1696\n",
      "Epoch: 070, Loss: 3.3406, Train: 1.1288, Val: 1.1412, Test: 1.1600\n",
      "Epoch: 071, Loss: 3.3228, Train: 1.1169, Val: 1.1306, Test: 1.1486\n",
      "Epoch: 072, Loss: 3.3079, Train: 1.1113, Val: 1.1262, Test: 1.1437\n",
      "Epoch: 073, Loss: 3.2970, Train: 1.1167, Val: 1.1324, Test: 1.1500\n",
      "Epoch: 074, Loss: 3.2818, Train: 1.1299, Val: 1.1459, Test: 1.1640\n",
      "Epoch: 075, Loss: 3.2656, Train: 1.1414, Val: 1.1577, Test: 1.1760\n",
      "Epoch: 076, Loss: 3.2545, Train: 1.1422, Val: 1.1593, Test: 1.1774\n",
      "Epoch: 077, Loss: 3.2413, Train: 1.1331, Val: 1.1516, Test: 1.1690\n",
      "Epoch: 078, Loss: 3.2256, Train: 1.1222, Val: 1.1424, Test: 1.1588\n",
      "Epoch: 079, Loss: 3.2137, Train: 1.1180, Val: 1.1395, Test: 1.1554\n",
      "Epoch: 080, Loss: 3.2033, Train: 1.1229, Val: 1.1448, Test: 1.1609\n",
      "Epoch: 081, Loss: 3.1896, Train: 1.1326, Val: 1.1546, Test: 1.1714\n",
      "Epoch: 082, Loss: 3.1774, Train: 1.1389, Val: 1.1613, Test: 1.1785\n",
      "Epoch: 083, Loss: 3.1674, Train: 1.1365, Val: 1.1599, Test: 1.1771\n",
      "Epoch: 084, Loss: 3.1555, Train: 1.1279, Val: 1.1527, Test: 1.1695\n",
      "Epoch: 085, Loss: 3.1441, Train: 1.1210, Val: 1.1468, Test: 1.1632\n",
      "Epoch: 086, Loss: 3.1338, Train: 1.1196, Val: 1.1457, Test: 1.1623\n",
      "Epoch: 087, Loss: 3.1230, Train: 1.1224, Val: 1.1484, Test: 1.1654\n",
      "Epoch: 088, Loss: 3.1134, Train: 1.1243, Val: 1.1506, Test: 1.1680\n",
      "Epoch: 089, Loss: 3.1042, Train: 1.1223, Val: 1.1495, Test: 1.1670\n",
      "Epoch: 090, Loss: 3.0944, Train: 1.1183, Val: 1.1464, Test: 1.1638\n",
      "Epoch: 091, Loss: 3.0863, Train: 1.1163, Val: 1.1450, Test: 1.1625\n",
      "Epoch: 092, Loss: 3.0776, Train: 1.1165, Val: 1.1453, Test: 1.1630\n",
      "Epoch: 093, Loss: 3.0688, Train: 1.1156, Val: 1.1446, Test: 1.1625\n",
      "Epoch: 094, Loss: 3.0609, Train: 1.1128, Val: 1.1425, Test: 1.1605\n",
      "Epoch: 095, Loss: 3.0529, Train: 1.1101, Val: 1.1406, Test: 1.1586\n",
      "Epoch: 096, Loss: 3.0454, Train: 1.1105, Val: 1.1413, Test: 1.1596\n",
      "Epoch: 097, Loss: 3.0376, Train: 1.1124, Val: 1.1434, Test: 1.1620\n",
      "Epoch: 098, Loss: 3.0302, Train: 1.1125, Val: 1.1437, Test: 1.1625\n",
      "Epoch: 099, Loss: 3.0234, Train: 1.1096, Val: 1.1414, Test: 1.1603\n",
      "Epoch: 100, Loss: 3.0165, Train: 1.1074, Val: 1.1396, Test: 1.1587\n",
      "Epoch: 101, Loss: 3.0099, Train: 1.1073, Val: 1.1397, Test: 1.1590\n",
      "Epoch: 102, Loss: 3.0033, Train: 1.1083, Val: 1.1409, Test: 1.1604\n",
      "Epoch: 103, Loss: 2.9971, Train: 1.1083, Val: 1.1413, Test: 1.1609\n",
      "Epoch: 104, Loss: 2.9909, Train: 1.1078, Val: 1.1412, Test: 1.1607\n",
      "Epoch: 105, Loss: 2.9850, Train: 1.1073, Val: 1.1410, Test: 1.1605\n",
      "Epoch: 106, Loss: 2.9791, Train: 1.1058, Val: 1.1399, Test: 1.1593\n",
      "Epoch: 107, Loss: 2.9736, Train: 1.1052, Val: 1.1395, Test: 1.1589\n",
      "Epoch: 108, Loss: 2.9684, Train: 1.1062, Val: 1.1405, Test: 1.1599\n",
      "Epoch: 109, Loss: 2.9631, Train: 1.1060, Val: 1.1406, Test: 1.1600\n",
      "Epoch: 110, Loss: 2.9581, Train: 1.1060, Val: 1.1405, Test: 1.1600\n",
      "Epoch: 111, Loss: 2.9532, Train: 1.1047, Val: 1.1394, Test: 1.1587\n",
      "Epoch: 112, Loss: 2.9485, Train: 1.1023, Val: 1.1376, Test: 1.1568\n",
      "Epoch: 113, Loss: 2.9442, Train: 1.1047, Val: 1.1395, Test: 1.1587\n",
      "Epoch: 114, Loss: 2.9401, Train: 1.1030, Val: 1.1385, Test: 1.1576\n",
      "Epoch: 115, Loss: 2.9353, Train: 1.1025, Val: 1.1384, Test: 1.1574\n",
      "Epoch: 116, Loss: 2.9315, Train: 1.1048, Val: 1.1401, Test: 1.1592\n",
      "Epoch: 117, Loss: 2.9279, Train: 1.1014, Val: 1.1376, Test: 1.1565\n",
      "Epoch: 118, Loss: 2.9233, Train: 1.0988, Val: 1.1358, Test: 1.1544\n",
      "Epoch: 119, Loss: 2.9207, Train: 1.1038, Val: 1.1398, Test: 1.1587\n",
      "Epoch: 120, Loss: 2.9170, Train: 1.1023, Val: 1.1389, Test: 1.1577\n",
      "Epoch: 121, Loss: 2.9127, Train: 1.0976, Val: 1.1355, Test: 1.1540\n",
      "Epoch: 122, Loss: 2.9109, Train: 1.1013, Val: 1.1385, Test: 1.1574\n",
      "Epoch: 123, Loss: 2.9057, Train: 1.1035, Val: 1.1402, Test: 1.1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Loss: 2.9037, Train: 1.0978, Val: 1.1358, Test: 1.1545\n",
      "Epoch: 125, Loss: 2.8992, Train: 1.0966, Val: 1.1351, Test: 1.1538\n",
      "Epoch: 126, Loss: 2.8964, Train: 1.1018, Val: 1.1394, Test: 1.1585\n",
      "Epoch: 127, Loss: 2.8936, Train: 1.1007, Val: 1.1388, Test: 1.1579\n",
      "Epoch: 128, Loss: 2.8898, Train: 1.0969, Val: 1.1359, Test: 1.1549\n",
      "Epoch: 129, Loss: 2.8879, Train: 1.0994, Val: 1.1378, Test: 1.1570\n",
      "Epoch: 130, Loss: 2.8843, Train: 1.0982, Val: 1.1368, Test: 1.1561\n",
      "Epoch: 131, Loss: 2.8812, Train: 1.0954, Val: 1.1348, Test: 1.1541\n",
      "Epoch: 132, Loss: 2.8791, Train: 1.0991, Val: 1.1378, Test: 1.1573\n",
      "Epoch: 133, Loss: 2.8755, Train: 1.0988, Val: 1.1376, Test: 1.1572\n",
      "Epoch: 134, Loss: 2.8727, Train: 1.0946, Val: 1.1343, Test: 1.1537\n",
      "Epoch: 135, Loss: 2.8703, Train: 1.0970, Val: 1.1363, Test: 1.1557\n",
      "Epoch: 136, Loss: 2.8673, Train: 1.0965, Val: 1.1362, Test: 1.1556\n",
      "Epoch: 137, Loss: 2.8643, Train: 1.0959, Val: 1.1359, Test: 1.1553\n",
      "Epoch: 138, Loss: 2.8620, Train: 1.0977, Val: 1.1375, Test: 1.1569\n",
      "Epoch: 139, Loss: 2.8596, Train: 1.0941, Val: 1.1347, Test: 1.1540\n",
      "Epoch: 140, Loss: 2.8567, Train: 1.0946, Val: 1.1353, Test: 1.1546\n",
      "Epoch: 141, Loss: 2.8539, Train: 1.0957, Val: 1.1364, Test: 1.1558\n",
      "Epoch: 142, Loss: 2.8514, Train: 1.0958, Val: 1.1367, Test: 1.1562\n",
      "Epoch: 143, Loss: 2.8490, Train: 1.0940, Val: 1.1354, Test: 1.1548\n",
      "Epoch: 144, Loss: 2.8468, Train: 1.0959, Val: 1.1370, Test: 1.1564\n",
      "Epoch: 145, Loss: 2.8457, Train: 1.0920, Val: 1.1342, Test: 1.1535\n",
      "Epoch: 146, Loss: 2.8428, Train: 1.0958, Val: 1.1375, Test: 1.1569\n",
      "Epoch: 147, Loss: 2.8400, Train: 1.0949, Val: 1.1370, Test: 1.1565\n",
      "Epoch: 148, Loss: 2.8375, Train: 1.0935, Val: 1.1359, Test: 1.1552\n",
      "Epoch: 149, Loss: 2.8352, Train: 1.0928, Val: 1.1354, Test: 1.1547\n",
      "Epoch: 150, Loss: 2.8332, Train: 1.0915, Val: 1.1347, Test: 1.1539\n",
      "Epoch: 151, Loss: 2.8318, Train: 1.0978, Val: 1.1398, Test: 1.1594\n",
      "Epoch: 152, Loss: 2.8308, Train: 1.0912, Val: 1.1347, Test: 1.1539\n",
      "Epoch: 153, Loss: 2.8270, Train: 1.0911, Val: 1.1348, Test: 1.1540\n",
      "Epoch: 154, Loss: 2.8245, Train: 1.0947, Val: 1.1378, Test: 1.1573\n",
      "Epoch: 155, Loss: 2.8227, Train: 1.0919, Val: 1.1358, Test: 1.1552\n",
      "Epoch: 156, Loss: 2.8209, Train: 1.0937, Val: 1.1372, Test: 1.1567\n",
      "Epoch: 157, Loss: 2.8190, Train: 1.0894, Val: 1.1340, Test: 1.1532\n",
      "Epoch: 158, Loss: 2.8164, Train: 1.0926, Val: 1.1367, Test: 1.1562\n",
      "Epoch: 159, Loss: 2.8139, Train: 1.0932, Val: 1.1375, Test: 1.1570\n",
      "Epoch: 160, Loss: 2.8118, Train: 1.0909, Val: 1.1358, Test: 1.1551\n",
      "Epoch: 161, Loss: 2.8097, Train: 1.0899, Val: 1.1351, Test: 1.1543\n",
      "Epoch: 162, Loss: 2.8076, Train: 1.0912, Val: 1.1364, Test: 1.1557\n",
      "Epoch: 163, Loss: 2.8056, Train: 1.0931, Val: 1.1381, Test: 1.1575\n",
      "Epoch: 164, Loss: 2.8043, Train: 1.0873, Val: 1.1338, Test: 1.1529\n",
      "Epoch: 165, Loss: 2.8049, Train: 1.0960, Val: 1.1405, Test: 1.1601\n",
      "Epoch: 166, Loss: 2.8038, Train: 1.0879, Val: 1.1342, Test: 1.1531\n",
      "Epoch: 167, Loss: 2.7985, Train: 1.0840, Val: 1.1316, Test: 1.1503\n",
      "Epoch: 168, Loss: 2.8018, Train: 1.1011, Val: 1.1455, Test: 1.1655\n",
      "Epoch: 169, Loss: 2.7974, Train: 1.0913, Val: 1.1373, Test: 1.1565\n",
      "Epoch: 170, Loss: 2.7932, Train: 1.0772, Val: 1.1261, Test: 1.1441\n",
      "Epoch: 171, Loss: 2.7955, Train: 1.0947, Val: 1.1406, Test: 1.1601\n",
      "Epoch: 172, Loss: 2.7893, Train: 1.0988, Val: 1.1442, Test: 1.1640\n",
      "Epoch: 173, Loss: 2.7888, Train: 1.0807, Val: 1.1296, Test: 1.1481\n",
      "Epoch: 174, Loss: 2.7874, Train: 1.0863, Val: 1.1341, Test: 1.1529\n",
      "Epoch: 175, Loss: 2.7826, Train: 1.0964, Val: 1.1425, Test: 1.1621\n",
      "Epoch: 176, Loss: 2.7830, Train: 1.0859, Val: 1.1343, Test: 1.1532\n",
      "Epoch: 177, Loss: 2.7794, Train: 1.0852, Val: 1.1338, Test: 1.1525\n",
      "Epoch: 178, Loss: 2.7766, Train: 1.0911, Val: 1.1386, Test: 1.1577\n",
      "Epoch: 179, Loss: 2.7757, Train: 1.0863, Val: 1.1352, Test: 1.1539\n",
      "Epoch: 180, Loss: 2.7725, Train: 1.0854, Val: 1.1346, Test: 1.1533\n",
      "Epoch: 181, Loss: 2.7708, Train: 1.0910, Val: 1.1391, Test: 1.1581\n",
      "Epoch: 182, Loss: 2.7697, Train: 1.0843, Val: 1.1339, Test: 1.1524\n",
      "Epoch: 183, Loss: 2.7665, Train: 1.0841, Val: 1.1339, Test: 1.1525\n",
      "Epoch: 184, Loss: 2.7650, Train: 1.0923, Val: 1.1406, Test: 1.1596\n",
      "Epoch: 185, Loss: 2.7643, Train: 1.0834, Val: 1.1336, Test: 1.1520\n",
      "Epoch: 186, Loss: 2.7606, Train: 1.0816, Val: 1.1325, Test: 1.1509\n",
      "Epoch: 187, Loss: 2.7599, Train: 1.0945, Val: 1.1431, Test: 1.1620\n",
      "Epoch: 188, Loss: 2.7601, Train: 1.0824, Val: 1.1334, Test: 1.1515\n",
      "Epoch: 189, Loss: 2.7547, Train: 1.0781, Val: 1.1302, Test: 1.1484\n",
      "Epoch: 190, Loss: 2.7563, Train: 1.0978, Val: 1.1466, Test: 1.1658\n",
      "Epoch: 191, Loss: 2.7549, Train: 1.0835, Val: 1.1348, Test: 1.1529\n",
      "Epoch: 192, Loss: 2.7489, Train: 1.0733, Val: 1.1268, Test: 1.1444\n",
      "Epoch: 193, Loss: 2.7508, Train: 1.0960, Val: 1.1457, Test: 1.1649\n",
      "Epoch: 194, Loss: 2.7474, Train: 1.0885, Val: 1.1395, Test: 1.1582\n",
      "Epoch: 195, Loss: 2.7429, Train: 1.0714, Val: 1.1256, Test: 1.1433\n",
      "Epoch: 196, Loss: 2.7448, Train: 1.0901, Val: 1.1411, Test: 1.1601\n",
      "Epoch: 197, Loss: 2.7394, Train: 1.0905, Val: 1.1416, Test: 1.1608\n",
      "Epoch: 198, Loss: 2.7370, Train: 1.0733, Val: 1.1276, Test: 1.1458\n",
      "Epoch: 199, Loss: 2.7379, Train: 1.0871, Val: 1.1389, Test: 1.1578\n",
      "Epoch: 200, Loss: 2.7330, Train: 1.0871, Val: 1.1391, Test: 1.1580\n",
      "Epoch: 201, Loss: 2.7303, Train: 1.0747, Val: 1.1292, Test: 1.1476\n",
      "Epoch: 202, Loss: 2.7303, Train: 1.0875, Val: 1.1399, Test: 1.1590\n",
      "Epoch: 203, Loss: 2.7258, Train: 1.0850, Val: 1.1380, Test: 1.1568\n",
      "Epoch: 204, Loss: 2.7235, Train: 1.0734, Val: 1.1287, Test: 1.1470\n",
      "Epoch: 205, Loss: 2.7230, Train: 1.0876, Val: 1.1406, Test: 1.1598\n",
      "Epoch: 206, Loss: 2.7192, Train: 1.0843, Val: 1.1380, Test: 1.1568\n",
      "Epoch: 207, Loss: 2.7164, Train: 1.0728, Val: 1.1287, Test: 1.1468\n",
      "Epoch: 208, Loss: 2.7155, Train: 1.0856, Val: 1.1394, Test: 1.1583\n",
      "Epoch: 209, Loss: 2.7123, Train: 1.0825, Val: 1.1372, Test: 1.1561\n",
      "Epoch: 210, Loss: 2.7096, Train: 1.0768, Val: 1.1326, Test: 1.1508\n",
      "Epoch: 211, Loss: 2.7071, Train: 1.0809, Val: 1.1362, Test: 1.1546\n",
      "Epoch: 212, Loss: 2.7048, Train: 1.0805, Val: 1.1363, Test: 1.1549\n",
      "Epoch: 213, Loss: 2.7027, Train: 1.0812, Val: 1.1371, Test: 1.1553\n",
      "Epoch: 214, Loss: 2.7001, Train: 1.0756, Val: 1.1328, Test: 1.1507\n",
      "Epoch: 215, Loss: 2.6977, Train: 1.0822, Val: 1.1385, Test: 1.1568\n",
      "Epoch: 216, Loss: 2.6951, Train: 1.0792, Val: 1.1363, Test: 1.1544\n",
      "Epoch: 217, Loss: 2.6925, Train: 1.0754, Val: 1.1334, Test: 1.1511\n",
      "Epoch: 218, Loss: 2.6903, Train: 1.0819, Val: 1.1390, Test: 1.1570\n",
      "Epoch: 219, Loss: 2.6884, Train: 1.0755, Val: 1.1341, Test: 1.1521\n",
      "Epoch: 220, Loss: 2.6870, Train: 1.0831, Val: 1.1404, Test: 1.1582\n",
      "Epoch: 221, Loss: 2.6853, Train: 1.0723, Val: 1.1321, Test: 1.1491\n",
      "Epoch: 222, Loss: 2.6814, Train: 1.0784, Val: 1.1374, Test: 1.1554\n",
      "Epoch: 223, Loss: 2.6796, Train: 1.0853, Val: 1.1432, Test: 1.1612\n",
      "Epoch: 224, Loss: 2.6777, Train: 1.0653, Val: 1.1270, Test: 1.1435\n",
      "Epoch: 225, Loss: 2.6766, Train: 1.0842, Val: 1.1427, Test: 1.1608\n",
      "Epoch: 226, Loss: 2.6723, Train: 1.0769, Val: 1.1369, Test: 1.1549\n",
      "Epoch: 227, Loss: 2.6693, Train: 1.0710, Val: 1.1323, Test: 1.1493\n",
      "Epoch: 228, Loss: 2.6667, Train: 1.0795, Val: 1.1394, Test: 1.1572\n",
      "Epoch: 229, Loss: 2.6642, Train: 1.0728, Val: 1.1342, Test: 1.1520\n",
      "Epoch: 230, Loss: 2.6630, Train: 1.0799, Val: 1.1402, Test: 1.1580\n",
      "Epoch: 231, Loss: 2.6598, Train: 1.0688, Val: 1.1314, Test: 1.1483\n",
      "Epoch: 232, Loss: 2.6571, Train: 1.0781, Val: 1.1393, Test: 1.1571\n",
      "Epoch: 233, Loss: 2.6542, Train: 1.0758, Val: 1.1377, Test: 1.1551\n",
      "Epoch: 234, Loss: 2.6512, Train: 1.0672, Val: 1.1311, Test: 1.1476\n",
      "Epoch: 235, Loss: 2.6496, Train: 1.0812, Val: 1.1428, Test: 1.1604\n",
      "Epoch: 236, Loss: 2.6474, Train: 1.0668, Val: 1.1313, Test: 1.1480\n",
      "Epoch: 237, Loss: 2.6458, Train: 1.0808, Val: 1.1428, Test: 1.1600\n",
      "Epoch: 238, Loss: 2.6442, Train: 1.0643, Val: 1.1299, Test: 1.1457\n",
      "Epoch: 239, Loss: 2.6403, Train: 1.0784, Val: 1.1418, Test: 1.1591\n",
      "Epoch: 240, Loss: 2.6375, Train: 1.0721, Val: 1.1369, Test: 1.1535\n",
      "Epoch: 241, Loss: 2.6333, Train: 1.0653, Val: 1.1316, Test: 1.1475\n",
      "Epoch: 242, Loss: 2.6322, Train: 1.0800, Val: 1.1441, Test: 1.1613\n",
      "Epoch: 243, Loss: 2.6298, Train: 1.0632, Val: 1.1307, Test: 1.1467\n",
      "Epoch: 244, Loss: 2.6280, Train: 1.0814, Val: 1.1456, Test: 1.1624\n",
      "Epoch: 245, Loss: 2.6285, Train: 1.0578, Val: 1.1271, Test: 1.1419\n",
      "Epoch: 246, Loss: 2.6250, Train: 1.0829, Val: 1.1479, Test: 1.1654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247, Loss: 2.6227, Train: 1.0632, Val: 1.1321, Test: 1.1478\n",
      "Epoch: 248, Loss: 2.6164, Train: 1.0722, Val: 1.1393, Test: 1.1551\n",
      "Epoch: 249, Loss: 2.6161, Train: 1.0675, Val: 1.1361, Test: 1.1517\n",
      "Epoch: 250, Loss: 2.6106, Train: 1.0663, Val: 1.1357, Test: 1.1521\n",
      "Epoch: 251, Loss: 2.6117, Train: 1.0806, Val: 1.1477, Test: 1.1645\n",
      "Epoch: 252, Loss: 2.6076, Train: 1.0497, Val: 1.1229, Test: 1.1365\n",
      "Epoch: 253, Loss: 2.6131, Train: 1.0988, Val: 1.1635, Test: 1.1812\n",
      "Epoch: 254, Loss: 2.6169, Train: 1.0398, Val: 1.1163, Test: 1.1294\n",
      "Epoch: 255, Loss: 2.6264, Train: 1.1162, Val: 1.1786, Test: 1.1971\n",
      "Epoch: 256, Loss: 2.6350, Train: 1.0335, Val: 1.1119, Test: 1.1230\n",
      "Epoch: 257, Loss: 2.6292, Train: 1.0979, Val: 1.1637, Test: 1.1813\n",
      "Epoch: 258, Loss: 2.6085, Train: 1.0568, Val: 1.1303, Test: 1.1453\n",
      "Epoch: 259, Loss: 2.5915, Train: 1.0562, Val: 1.1300, Test: 1.1446\n",
      "Epoch: 260, Loss: 2.5862, Train: 1.0905, Val: 1.1581, Test: 1.1750\n",
      "Epoch: 261, Loss: 2.5966, Train: 1.0400, Val: 1.1179, Test: 1.1304\n",
      "Epoch: 262, Loss: 2.5983, Train: 1.0938, Val: 1.1618, Test: 1.1796\n",
      "Epoch: 263, Loss: 2.5925, Train: 1.0513, Val: 1.1275, Test: 1.1419\n",
      "Epoch: 264, Loss: 2.5803, Train: 1.0649, Val: 1.1386, Test: 1.1533\n",
      "Epoch: 265, Loss: 2.5726, Train: 1.0721, Val: 1.1447, Test: 1.1599\n",
      "Epoch: 266, Loss: 2.5723, Train: 1.0480, Val: 1.1260, Test: 1.1395\n",
      "Epoch: 267, Loss: 2.5728, Train: 1.0875, Val: 1.1584, Test: 1.1754\n",
      "Epoch: 268, Loss: 2.5750, Train: 1.0444, Val: 1.1241, Test: 1.1368\n",
      "Epoch: 269, Loss: 2.5706, Train: 1.0807, Val: 1.1535, Test: 1.1693\n",
      "Epoch: 270, Loss: 2.5660, Train: 1.0524, Val: 1.1313, Test: 1.1446\n",
      "Epoch: 271, Loss: 2.5582, Train: 1.0653, Val: 1.1420, Test: 1.1570\n",
      "Epoch: 272, Loss: 2.5544, Train: 1.0679, Val: 1.1445, Test: 1.1594\n",
      "Epoch: 273, Loss: 2.5511, Train: 1.0491, Val: 1.1296, Test: 1.1423\n",
      "Epoch: 274, Loss: 2.5526, Train: 1.0822, Val: 1.1566, Test: 1.1724\n",
      "Epoch: 275, Loss: 2.5540, Train: 1.0385, Val: 1.1221, Test: 1.1342\n",
      "Epoch: 276, Loss: 2.5592, Train: 1.1001, Val: 1.1721, Test: 1.1891\n",
      "Epoch: 277, Loss: 2.5678, Train: 1.0266, Val: 1.1136, Test: 1.1233\n",
      "Epoch: 278, Loss: 2.5763, Train: 1.1146, Val: 1.1843, Test: 1.2023\n",
      "Epoch: 279, Loss: 2.5850, Train: 1.0249, Val: 1.1124, Test: 1.1224\n",
      "Epoch: 280, Loss: 2.5729, Train: 1.0937, Val: 1.1668, Test: 1.1837\n",
      "Epoch: 281, Loss: 2.5547, Train: 1.0427, Val: 1.1255, Test: 1.1380\n",
      "Epoch: 282, Loss: 2.5334, Train: 1.0509, Val: 1.1320, Test: 1.1455\n",
      "Epoch: 283, Loss: 2.5265, Train: 1.0803, Val: 1.1560, Test: 1.1721\n",
      "Epoch: 284, Loss: 2.5344, Train: 1.0291, Val: 1.1155, Test: 1.1264\n",
      "Epoch: 285, Loss: 2.5446, Train: 1.0992, Val: 1.1722, Test: 1.1892\n",
      "Epoch: 286, Loss: 2.5509, Train: 1.0286, Val: 1.1159, Test: 1.1262\n",
      "Epoch: 287, Loss: 2.5396, Train: 1.0773, Val: 1.1549, Test: 1.1704\n",
      "Epoch: 288, Loss: 2.5236, Train: 1.0507, Val: 1.1336, Test: 1.1469\n",
      "Epoch: 289, Loss: 2.5126, Train: 1.0460, Val: 1.1301, Test: 1.1428\n",
      "Epoch: 290, Loss: 2.5112, Train: 1.0780, Val: 1.1560, Test: 1.1719\n",
      "Epoch: 291, Loss: 2.5171, Train: 1.0323, Val: 1.1201, Test: 1.1315\n",
      "Epoch: 292, Loss: 2.5210, Train: 1.0905, Val: 1.1672, Test: 1.1842\n",
      "Epoch: 293, Loss: 2.5241, Train: 1.0293, Val: 1.1186, Test: 1.1294\n",
      "Epoch: 294, Loss: 2.5195, Train: 1.0825, Val: 1.1611, Test: 1.1771\n",
      "Epoch: 295, Loss: 2.5135, Train: 1.0362, Val: 1.1242, Test: 1.1358\n",
      "Epoch: 296, Loss: 2.5029, Train: 1.0618, Val: 1.1445, Test: 1.1593\n",
      "Epoch: 297, Loss: 2.4943, Train: 1.0532, Val: 1.1378, Test: 1.1519\n",
      "Epoch: 298, Loss: 2.4904, Train: 1.0439, Val: 1.1305, Test: 1.1435\n",
      "Epoch: 299, Loss: 2.4896, Train: 1.0692, Val: 1.1512, Test: 1.1665\n",
      "Epoch: 300, Loss: 2.4920, Train: 1.0306, Val: 1.1210, Test: 1.1324\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_Predic(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = test_Predic(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prediction==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=test_data[\"user\", \"rates\", \"movie\"]['edge_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real[real==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = test_Predic(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(val_data[\"user\", \"rates\", \"movie\"]['edge_label'], Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   5,  104,  596,  ...,  493,   90,  466],\n",
       "        [ 320, 5715,  686,  ...,  793, 2441, 1586]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[\"user\", \"rates\", \"movie\"]['edge_label_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_rating=np.array(list(rating[\"rating\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_rating[org_rating==0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=data[\"user\", \"rates\", \"movie\"]['edge_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1370"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real[real==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>1370</td>\n",
       "      <td>1370</td>\n",
       "      <td>1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2811</td>\n",
       "      <td>2811</td>\n",
       "      <td>2811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>1791</td>\n",
       "      <td>1791</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>7551</td>\n",
       "      <td>7551</td>\n",
       "      <td>7551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <td>5550</td>\n",
       "      <td>5550</td>\n",
       "      <td>5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>20047</td>\n",
       "      <td>20047</td>\n",
       "      <td>20047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.5</th>\n",
       "      <td>13136</td>\n",
       "      <td>13136</td>\n",
       "      <td>13136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>26818</td>\n",
       "      <td>26818</td>\n",
       "      <td>26818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.5</th>\n",
       "      <td>8551</td>\n",
       "      <td>8551</td>\n",
       "      <td>8551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>13211</td>\n",
       "      <td>13211</td>\n",
       "      <td>13211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  timestamp\n",
       "rating                            \n",
       "0.5       1370     1370       1370\n",
       "1.0       2811     2811       2811\n",
       "1.5       1791     1791       1791\n",
       "2.0       7551     7551       7551\n",
       "2.5       5550     5550       5550\n",
       "3.0      20047    20047      20047\n",
       "3.5      13136    13136      13136\n",
       "4.0      26818    26818      26818\n",
       "4.5       8551     8551       8551\n",
       "5.0      13211    13211      13211"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.groupby(\"rating\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
       "  \u001b[1muser\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 100836],\n",
       "    edge_label=[100836]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 100836] }\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016968273253211548"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100836/(9742*610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
